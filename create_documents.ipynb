{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "import os\n",
    "import psycopg2 as pg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to postgresql database\n",
    "\n",
    "connection = pg.connect(\n",
    "    user=os.environ.get('DB_USER'),\n",
    "    password=os.environ.get('DB_PASSWORD'),\n",
    "    host=\"localhost\",\n",
    "    port=os.environ.get('DB_PORT'),\n",
    "    database=\"vector_test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_pdf = PyPDFLoader(\"documents/Perigo_James_Courtney_resume_2023_nocontactinfo.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = resume_pdf.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\jarvis-ai-demo\\.venv\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "text_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    tokens_per_chunk=20,\n",
    "    chunk_overlap=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.transform_documents(resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='james “ courtney ” perigo expert data science leader driving innovation for business success contact courtneyperigo.', metadata={'source': 'documents/Perigo_James_Courtney_resume_2023_nocontactinfo.pdf', 'page': 0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc in docs:\n",
    "#     source = doc.metadata['source']\n",
    "#     page = doc.metadata['page']\n",
    "#     page_content = doc.page_content\n",
    "#     vector = embeddings.embed_query(page_content)\n",
    "#     # write to database\n",
    "#     cursor = connection.cursor()\n",
    "#     cursor.execute(\"INSERT INTO t_docs (doc_source, doc_page, embedding, doc_content) VALUES (%s, %s, %s, %s)\", (source, page, vector, page_content))\n",
    "#     connection.commit()\n",
    "#     cursor.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "# check if cuda is available\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b_lora\"\n",
    "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n",
    "DEFAULT_RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"Imagine you are a helpful assistant answering questions about Courtney Perigo. You are to answer truthfully and accurately.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=DEFAULT_MESSAGE_TEMPLATE,\n",
    "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "        response_template=DEFAULT_RESPONSE_TEMPLATE\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.response_template = response_template\n",
    "        self.messages = [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        }]\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"bot\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += DEFAULT_RESPONSE_TEMPLATE\n",
    "        return final_text.strip()\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(\n",
    "        **data,\n",
    "        generation_config=generation_config\n",
    "    )[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin d:\\Documents\\GitHub\\jarvis-ai-demo\\.venv\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118_nocublaslt.dll\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bee36000ff4b7b80e0cb4b4854b174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_new_tokens\": 1536,\n",
      "  \"no_repeat_ngram_size\": 15,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.2,\n",
      "  \"top_k\": 40,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "print(generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Does Courtney have a dog?\"\n",
    "query_vector = embeddings.embed_query(user_query)\n",
    "\n",
    "query_vector_db = f\"\"\"SELECT * from t_docs ORDER BY embedding <-> '{query_vector}' LIMIT 5\"\"\"\n",
    "\n",
    "# get closest documents that match a query\n",
    "\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(query_vector_db)\n",
    "\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "cursor.close()\n",
    "\n",
    "text = f\"\"\"\n",
    "    Imagine you are a helpful assistant answering questions about a man named Courtney Perigo. \n",
    "    You are to answer truthfully and accurately.  \n",
    "    Your response will be professional and not contain profanity or any other inappropriate content. \n",
    "    Use the following 5 pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "    Keep the answer as concise as possible. \n",
    "    1. {rows[0][3]} \n",
    "    2. {rows[1][3]} \n",
    "    3. {rows[2][3]} \n",
    "    4. {rows[3][3]} \n",
    "    5. {rows[4][3]} \n",
    "    Question: {user_query}\n",
    "    Helpful Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Prompt---\n",
      "\n",
      "    Imagine you are a helpful assistant answering questions about a man named Courtney Perigo. \n",
      "    You are to answer truthfully and accurately.  \n",
      "    Your response will be professional and not contain profanity or any other inappropriate content. \n",
      "    Use the following 5 pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
      "    Keep the answer as concise as possible. \n",
      "    1. courtney - perigo. medium. com profile data s cience l eader and p ract \n",
      "    2. james “ courtney ” perigo expert data science leader driving innovation for business success contact courtneyperigo. \n",
      "    3. james “ courtney ” perigo expert data science leader driving innovation for business success experience, continued director, \n",
      "    4. in annual recurring revenue for the agency. won the drum digital advertising award for our novel c urve \n",
      "    5. in paid marketing performance. vp, director of decision sciences publicis, moxie | 2014 - 2018 \n",
      "    Question: Does Courtney have a dog?\n",
      "    Helpful Answer:\n",
      "\n",
      "---Response---\n",
      "I don't know.\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversation = Conversation()\n",
    "conversation.add_user_message(text)\n",
    "prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "output = generate(model, tokenizer, prompt, generation_config)\n",
    "print('---Prompt---')\n",
    "print(text)\n",
    "print('---Response---')\n",
    "print(output)\n",
    "print()\n",
    "print(\"==============================\")\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
